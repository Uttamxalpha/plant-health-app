"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘   PLANT HEALTH CNN â€” FULL RETRAINING PIPELINE               â•‘
â•‘   Dataset : kagglehub â†’ vipoooool/new-plant-diseases-dataset â•‘
â•‘   Run     : python train.py                                  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

What this script does, step by step:
  1.  Download dataset via kagglehub
  2.  Auto-discover dataset structure (train / valid folders)
  3.  Build tf.data pipelines with aggressive augmentation
  4.  Construct the CNN (residual + attention + depthwise sep)
  5.  Compile with cosine-decay LR + label smoothing
  6.  Train with early-stopping & model checkpointing
  7.  Evaluate on validation set
  8.  Plot & save training curves
  9.  Export model â†’ saved_models/plant_health_final.keras
  10. Save class_names.json for the Streamlit app
"""

# â”€â”€ Stdlib â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import os, sys, json, time, shutil, warnings
from pathlib import Path
from datetime import datetime

warnings.filterwarnings("ignore")
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "2"   # suppress TF C++ spam

# â”€â”€ Third-party â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import numpy as np
import matplotlib
matplotlib.use("Agg")                        # headless backend
import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# STEP 0 â€” Install & import kagglehub
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
try:
    import kagglehub
except ImportError:
    print("  Installing kagglehubâ€¦")
    os.system(f"{sys.executable} -m pip install kagglehub -q")
    import kagglehub

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, Model
from tensorflow.keras.callbacks import (
    EarlyStopping, ModelCheckpoint,
    ReduceLROnPlateau, TensorBoard, CSVLogger
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# CONFIGURATION
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class CFG:
    # â”€â”€ Paths â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    SAVE_DIR    = Path("saved_models")
    LOG_DIR     = Path("logs") / datetime.now().strftime("%Y%m%d_%H%M%S")
    PLOT_DIR    = Path("plots")

    # â”€â”€ Image â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    IMG_H       = 224
    IMG_W       = 224
    CHANNELS    = 3
    INPUT_SHAPE = (IMG_H, IMG_W, CHANNELS)

    # â”€â”€ Training â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    BATCH_SIZE  = 32
    EPOCHS      = 60
    LR_INIT     = 1e-3
    LR_MIN      = 1e-7
    DROPOUT     = 0.4
    L2          = 1e-4
    LABEL_SMOOTH= 0.1

    # â”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    USE_PRETRAINED = False   # True = MobileNetV2 backbone
    FINE_TUNE_AT   = -30     # unfreeze last N layers when USE_PRETRAINED

    # â”€â”€ Misc â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    SEED        = 42
    AUTOTUNE    = tf.data.AUTOTUNE

cfg = CFG()
tf.random.set_seed(cfg.SEED)
np.random.seed(cfg.SEED)

for d in [cfg.SAVE_DIR, cfg.LOG_DIR, cfg.PLOT_DIR]:
    d.mkdir(parents=True, exist_ok=True)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# STEP 1 â€” DOWNLOAD DATASET
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def download_dataset() -> Path:
    print("\n" + "â•"*60)
    print("  [1/9] Downloading dataset via KaggleHub")
    print("â•"*60)
    print("  Dataset : vipoooool/new-plant-diseases-dataset")
    print("  Note    : Requires Kaggle credentials (~2 GB download)")
    print("            Set KAGGLE_USERNAME & KAGGLE_KEY env vars,")
    print("            or place kaggle.json in ~/.kaggle/")
    print()

    t0 = time.time()
    path = kagglehub.dataset_download("vipoooool/new-plant-diseases-dataset")
    elapsed = time.time() - t0

    path = Path(path)
    print(f"\n  âœ… Downloaded in {elapsed:.1f}s")
    print(f"  ğŸ“ Path: {path}")
    return path


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# STEP 2 â€” DISCOVER DATASET STRUCTURE
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def find_data_dirs(root: Path):
    """
    The KaggleHub dataset has this structure:
      <root>/
        New Plant Diseases Dataset(Augmented)/
          New Plant Diseases Dataset(Augmented)/
            train/
              Apple___Apple_scab/  â€¦
            valid/
              Apple___Apple_scab/  â€¦
    We walk the tree to find the train & valid directories.
    """
    print("\n  [2/9] Discovering dataset structureâ€¦")

    train_dir = valid_dir = None

    for p in sorted(root.rglob("*")):
        if p.is_dir():
            name = p.name.lower()
            if name == "train"  and train_dir is None:
                train_dir = p
            if name in ("valid", "val", "validation") and valid_dir is None:
                valid_dir = p

    if train_dir is None:
        raise FileNotFoundError(
            f"Could not find 'train' directory under {root}.\n"
            f"Directory tree:\n" + "\n".join(str(p) for p in root.rglob("*") if p.is_dir())
        )

    classes = sorted([d.name for d in train_dir.iterdir() if d.is_dir()])
    n_train = sum(len(list(d.glob("*.*"))) for d in train_dir.iterdir() if d.is_dir())
    n_valid = sum(len(list(d.glob("*.*"))) for d in valid_dir.iterdir() if d.is_dir()) if valid_dir else 0

    print(f"  âœ… train dir  : {train_dir}  ({n_train:,} images)")
    if valid_dir:
        print(f"  âœ… valid dir  : {valid_dir}  ({n_valid:,} images)")
    print(f"  âœ… classes    : {len(classes)}")

    return train_dir, valid_dir, classes


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# STEP 3 â€” BUILD tf.data PIPELINE
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def build_dataset(directory: Path, classes: list, training: bool) -> tf.data.Dataset:
    """
    Loads images from directory using keras utility, then wraps in
    a fast, cached, prefetched tf.data pipeline.

    Augmentation layers are baked into the dataset (training only)
    so augmentation runs on CPU in parallel while GPU trains.
    """
    ds = keras.utils.image_dataset_from_directory(
        str(directory),
        labels="inferred",
        label_mode="categorical",
        class_names=classes,
        image_size=(cfg.IMG_H, cfg.IMG_W),
        batch_size=cfg.BATCH_SIZE,
        shuffle=training,
        seed=cfg.SEED,
    )

    # Normalise [0,255] â†’ [0,1]
    normalise = layers.Rescaling(1.0 / 255.0)

    # Heavy augmentation pipeline (training only)
    augment = keras.Sequential([
        layers.RandomFlip("horizontal_and_vertical"),
        layers.RandomRotation(0.25),
        layers.RandomZoom(0.20),
        layers.RandomTranslation(0.10, 0.10),
        layers.RandomContrast(0.20),
        layers.RandomBrightness(0.15),
    ], name="augmentation")

    def preprocess_train(x, y):
        x = normalise(x)
        x = augment(x, training=True)
        return x, y

    def preprocess_val(x, y):
        x = normalise(x)
        return x, y

    if training:
        ds = ds.map(preprocess_train, num_parallel_calls=cfg.AUTOTUNE)
        ds = ds.shuffle(buffer_size=1000, seed=cfg.SEED, reshuffle_each_iteration=True)
        ds = ds.repeat()
    else:
        ds = ds.map(preprocess_val, num_parallel_calls=cfg.AUTOTUNE)

    ds = ds.prefetch(cfg.AUTOTUNE)
    return ds


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# STEP 4 â€” MODEL ARCHITECTURE
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# â”€â”€ Building-block helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def conv_bn_relu(x, filters, kernel=3, strides=1):
    x = layers.Conv2D(
        filters, kernel, strides=strides, padding="same",
        use_bias=False,
        kernel_regularizer=keras.regularizers.l2(cfg.L2),
        kernel_initializer="he_normal",
    )(x)
    x = layers.BatchNormalization()(x)
    x = layers.Activation("relu")(x)
    return x


def residual_block(x, filters):
    """Pre-activation residual block."""
    shortcut = x
    in_ch    = x.shape[-1]

    # Main path
    x = layers.BatchNormalization()(x)
    x = layers.Activation("relu")(x)
    x = layers.Conv2D(filters, 3, padding="same", use_bias=False,
                      kernel_regularizer=keras.regularizers.l2(cfg.L2))(x)

    x = layers.BatchNormalization()(x)
    x = layers.Activation("relu")(x)
    x = layers.Conv2D(filters, 3, padding="same", use_bias=False,
                      kernel_regularizer=keras.regularizers.l2(cfg.L2))(x)

    # Project shortcut if channel dims differ
    if in_ch != filters:
        shortcut = layers.Conv2D(filters, 1, padding="same", use_bias=False)(shortcut)

    return layers.Add()([x, shortcut])


def depthwise_sep_block(x, filters):
    """MobileNet-style depthwise separable conv block."""
    x = layers.DepthwiseConv2D(3, padding="same", use_bias=False,
                                depthwise_regularizer=keras.regularizers.l2(cfg.L2))(x)
    x = layers.BatchNormalization()(x)
    x = layers.Activation("relu")(x)
    x = layers.Conv2D(filters, 1, padding="same", use_bias=False,
                      kernel_regularizer=keras.regularizers.l2(cfg.L2))(x)
    x = layers.BatchNormalization()(x)
    x = layers.Activation("relu")(x)
    return x


def squeeze_excite(x, ratio=16):
    """Channel Attention â€” Squeeze-and-Excitation block."""
    ch  = x.shape[-1]
    gap = layers.GlobalAveragePooling2D(keepdims=True)(x)
    se  = layers.Dense(max(ch // ratio, 8), activation="relu",  use_bias=False)(gap)
    se  = layers.Dense(ch,                  activation="sigmoid", use_bias=False)(se)
    return layers.Multiply()([x, se])


def build_model(num_classes: int) -> Model:
    """
    PlantHealthCNN Architecture
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    Input  224Ã—224Ã—3
    Stage1  Stem  Conv(32, 3Ã—3, s=2) + Conv(32) + MaxPool  â†’ 56Ã—56Ã—32
    Stage2  2Ã— Conv(64)  + MaxPool                          â†’ 28Ã—28Ã—64
    Stage3  2Ã— ResBlock(128) + MaxPool                      â†’ 14Ã—14Ã—128
    Stage4  2Ã— ResBlock(256) + MaxPool                      â†’  7Ã—7Ã—256
    Stage5  2Ã— DWSep(512)                                   â†’  7Ã—7Ã—512
    Stage6  SE Channel Attention                            â†’  7Ã—7Ã—512
    Stage7  GlobalAveragePool                               â†’  512
    Head    Dense(512)â†’BNâ†’Drop(0.4)â†’Dense(256)â†’Drop(0.2)  â†’  256
    Output  Dense(num_classes, softmax)                     â†’  N
    """
    if cfg.USE_PRETRAINED:
        return _build_transfer_model(num_classes)

    inp = keras.Input(shape=cfg.INPUT_SHAPE, name="leaf_image")
    x   = inp

    # â”€â”€ Stage 1: Stem â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    x = conv_bn_relu(x, 32, kernel=3, strides=2)    # 112Ã—112
    x = conv_bn_relu(x, 32, kernel=3)
    x = layers.MaxPooling2D(2, strides=2)(x)         # 56Ã—56

    # â”€â”€ Stage 2: Feature Extraction â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    x = conv_bn_relu(x, 64, kernel=3)
    x = conv_bn_relu(x, 64, kernel=3)
    x = layers.MaxPooling2D(2, strides=2)(x)         # 28Ã—28

    # â”€â”€ Stage 3: Residual Blocks â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    x = residual_block(x, 128)
    x = residual_block(x, 128)
    x = layers.MaxPooling2D(2, strides=2)(x)         # 14Ã—14

    # â”€â”€ Stage 4: Deep Residual Blocks â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    x = residual_block(x, 256)
    x = residual_block(x, 256)
    x = layers.MaxPooling2D(2, strides=2)(x)         #  7Ã—7

    # â”€â”€ Stage 5: Lightweight Depthwise Separable â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    x = depthwise_sep_block(x, 512)
    x = depthwise_sep_block(x, 512)

    # â”€â”€ Stage 6: Channel Attention â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    x = squeeze_excite(x, ratio=16)

    # â”€â”€ Stage 7: Global Pooling â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    x = layers.GlobalAveragePooling2D(name="gap")(x)

    # â”€â”€ Classifier Head â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    x = layers.Dense(512, use_bias=False,
                     kernel_regularizer=keras.regularizers.l2(cfg.L2))(x)
    x = layers.BatchNormalization()(x)
    x = layers.Activation("relu")(x)
    x = layers.Dropout(cfg.DROPOUT)(x)

    x = layers.Dense(256, activation="relu",
                     kernel_regularizer=keras.regularizers.l2(cfg.L2))(x)
    x = layers.Dropout(cfg.DROPOUT / 2)(x)

    out = layers.Dense(num_classes, activation="softmax", name="predictions")(x)

    return Model(inp, out, name="PlantHealthCNN")


def _build_transfer_model(num_classes: int) -> Model:
    """MobileNetV2 transfer-learning variant."""
    base = keras.applications.MobileNetV2(
        input_shape=cfg.INPUT_SHAPE,
        include_top=False,
        weights="imagenet",
    )
    # Freeze all except last N layers
    for layer in base.layers[:cfg.FINE_TUNE_AT]:
        layer.trainable = False

    inp = keras.Input(shape=cfg.INPUT_SHAPE, name="leaf_image")
    x   = base(inp, training=False)
    x   = squeeze_excite(x, ratio=16)
    x   = layers.GlobalAveragePooling2D()(x)
    x   = layers.Dense(512, activation="relu",
                       kernel_regularizer=keras.regularizers.l2(cfg.L2))(x)
    x   = layers.BatchNormalization()(x)
    x   = layers.Dropout(cfg.DROPOUT)(x)
    out = layers.Dense(num_classes, activation="softmax", name="predictions")(x)
    return Model(inp, out, name="PlantHealthCNN_Transfer")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# STEP 5 â€” COMPILE
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def compile_model(model: Model, steps_per_epoch: int) -> Model:
    """
    Optimizer : Adam with cosine-decay LR + warm restarts
    Loss      : Categorical cross-entropy with label smoothing
    Metrics   : Accuracy, Top-3 accuracy, AUC
    """
    lr_schedule = keras.optimizers.schedules.CosineDecayRestarts(
        initial_learning_rate = cfg.LR_INIT,
        first_decay_steps     = steps_per_epoch * 5,
        t_mul                 = 2.0,
        m_mul                 = 0.9,
        alpha                 = cfg.LR_MIN,
    )
    model.compile(
        optimizer = keras.optimizers.Adam(learning_rate=lr_schedule, clipnorm=1.0),
        loss      = keras.losses.CategoricalCrossentropy(
                        label_smoothing=cfg.LABEL_SMOOTH),
        metrics   = [
            "accuracy",
            keras.metrics.TopKCategoricalAccuracy(k=3, name="top3_acc"),
            keras.metrics.AUC(name="auc"),
        ],
    )
    return model


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# STEP 6 â€” CALLBACKS
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def get_callbacks() -> list:
    return [
        EarlyStopping(
            monitor            = "val_accuracy",
            patience           = 12,
            restore_best_weights = True,
            min_delta          = 0.001,
            verbose            = 1,
        ),
        ModelCheckpoint(
            filepath           = str(cfg.SAVE_DIR / "best_model.keras"),
            monitor            = "val_accuracy",
            save_best_only     = True,
            verbose            = 1,
        ),
        ReduceLROnPlateau(
            monitor            = "val_loss",
            factor             = 0.5,
            patience           = 5,
            min_lr             = cfg.LR_MIN,
            verbose            = 1,
        ),
        TensorBoard(
            log_dir            = str(cfg.LOG_DIR),
            histogram_freq     = 1,
            update_freq        = "epoch",
        ),
        CSVLogger(
            filename           = str(cfg.SAVE_DIR / "training_log.csv"),
            append             = False,
        ),
    ]


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# STEP 7 â€” TRAIN
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def train(model, train_ds, val_ds, n_train, n_val):
    steps_per_epoch  = max(1, n_train  // cfg.BATCH_SIZE)
    validation_steps = max(1, n_val    // cfg.BATCH_SIZE)

    print(f"\n  Steps / epoch  : {steps_per_epoch}")
    print(f"  Validation steps: {validation_steps}")
    print(f"  Max epochs      : {cfg.EPOCHS}")
    print()

    history = model.fit(
        train_ds,
        epochs           = cfg.EPOCHS,
        steps_per_epoch  = steps_per_epoch,
        validation_data  = val_ds,
        validation_steps = validation_steps,
        callbacks        = get_callbacks(),
        verbose          = 1,
    )
    return history


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# STEP 8 â€” EVALUATE
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def evaluate(model, val_ds, n_val):
    print("\n  [8/9] Final evaluation on validation setâ€¦")
    steps = max(1, n_val // cfg.BATCH_SIZE)
    results = model.evaluate(val_ds, steps=steps, verbose=1)
    metrics = dict(zip(model.metrics_names, results))

    print("\n  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
    print(f"  â”‚  Accuracy  : {metrics.get('accuracy', 0)*100:6.2f}%        â”‚")
    print(f"  â”‚  Top-3 Acc : {metrics.get('top3_acc', 0)*100:6.2f}%        â”‚")
    print(f"  â”‚  AUC       : {metrics.get('auc', 0):.4f}          â”‚")
    print(f"  â”‚  Loss      : {metrics.get('loss', 0):.4f}          â”‚")
    print("  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
    return metrics


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# STEP 9 â€” PLOT & SAVE
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def plot_history(history, save_path: Path):
    """Save a 4-panel training history plot."""
    h   = history.history
    eps = range(1, len(h["accuracy"]) + 1)

    fig = plt.figure(figsize=(16, 10))
    fig.patch.set_facecolor("#0d1117")

    gs  = gridspec.GridSpec(2, 2, figure=fig, hspace=0.45, wspace=0.35)
    panels = [
        (gs[0, 0], "accuracy",  "val_accuracy", "Accuracy",   "#58a6ff", "#3fb950"),
        (gs[0, 1], "loss",      "val_loss",      "Loss",       "#f85149", "#d29922"),
        (gs[1, 0], "top3_acc",  "val_top3_acc",  "Top-3 Acc", "#79c0ff", "#56d364"),
        (gs[1, 1], "auc",       "val_auc",        "AUC",       "#cba6f7", "#f38ba8"),
    ]

    for spec, train_k, val_k, title, tc, vc in panels:
        ax = fig.add_subplot(spec)
        ax.set_facecolor("#161b22")
        ax.spines["bottom"].set_color("#30363d")
        ax.spines["left"].set_color("#30363d")
        ax.spines["top"].set_visible(False)
        ax.spines["right"].set_visible(False)
        ax.tick_params(colors="#8b949e", labelsize=9)
        ax.xaxis.label.set_color("#8b949e")
        ax.yaxis.label.set_color("#8b949e")
        ax.title.set_color("#f0f6fc")

        if train_k in h:
            ax.plot(eps, h[train_k], color=tc, lw=2, label="Train", alpha=0.9)
        if val_k in h:
            ax.plot(eps, h[val_k], color=vc, lw=2, label="Val", linestyle="--", alpha=0.9)

        # Mark best val epoch
        if val_k in h:
            best_e  = int(np.argmax(h[val_k]) if "acc" in val_k or "auc" in val_k
                         else np.argmin(h[val_k])) + 1
            best_v  = h[val_k][best_e - 1]
            ax.axvline(best_e, color="#6e7681", lw=1, linestyle=":")
            ax.annotate(f"best: {best_v:.4f}", xy=(best_e, best_v),
                        xytext=(5, 5), textcoords="offset points",
                        color="#8b949e", fontsize=7)

        ax.set_title(title, fontsize=12, fontweight="bold", pad=10)
        ax.set_xlabel("Epoch")
        ax.legend(facecolor="#21262d", labelcolor="#c9d1d9", fontsize=8,
                  framealpha=0.8, loc="best")
        ax.grid(True, color="#21262d", linewidth=0.5)

    fig.suptitle("Plant Health CNN â€” Training History",
                 color="#f0f6fc", fontsize=15, fontweight="bold", y=0.98)
    plt.savefig(save_path, dpi=150, bbox_inches="tight", facecolor="#0d1117")
    plt.close(fig)
    print(f"\n  ğŸ“Š Training plot saved: {save_path}")


def save_artifacts(model, classes, history, metrics):
    """Save model + class names + training summary."""
    # 1. Final model
    model_path = cfg.SAVE_DIR / "plant_health_final.keras"
    model.save(str(model_path))
    print(f"\n  âœ… Model saved      : {model_path}")

    # 2. TFLite (compressed, for edge deployment)
    try:
        converter = tf.lite.TFLiteConverter.from_keras_model(model)
        converter.optimizations = [tf.lite.Optimize.DEFAULT]
        tflite_model = converter.convert()
        tflite_path  = cfg.SAVE_DIR / "plant_health.tflite"
        tflite_path.write_bytes(tflite_model)
        print(f"  âœ… TFLite saved     : {tflite_path}")
    except Exception as e:
        print(f"  âš ï¸  TFLite export skipped: {e}")

    # 3. Class names (for Streamlit app)
    class_path = cfg.SAVE_DIR / "class_names.json"
    with open(class_path, "w") as f:
        json.dump(classes, f, indent=2)
    print(f"  âœ… Class names saved: {class_path}")

    # 4. Training summary
    summary = {
        "timestamp":   datetime.now().isoformat(),
        "num_classes": len(classes),
        "epochs_run":  len(history.history["accuracy"]),
        "best_val_acc": float(max(history.history.get("val_accuracy", [0]))),
        "best_top3":   float(max(history.history.get("val_top3_acc",  [0]))),
        "best_auc":    float(max(history.history.get("val_auc",        [0]))),
        "final_metrics": {k: float(v) for k, v in metrics.items()},
        "config": {
            "img_size":    [cfg.IMG_H, cfg.IMG_W],
            "batch_size":  cfg.BATCH_SIZE,
            "lr_init":     cfg.LR_INIT,
            "dropout":     cfg.DROPOUT,
            "use_pretrained": cfg.USE_PRETRAINED,
        }
    }
    summary_path = cfg.SAVE_DIR / "training_summary.json"
    with open(summary_path, "w") as f:
        json.dump(summary, f, indent=2)
    print(f"  âœ… Summary saved    : {summary_path}")

    return model_path


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# MAIN
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def main():
    print("\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    print("â•‘   PLANT HEALTH CNN â€” FULL RETRAINING PIPELINE       â•‘")
    print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
    print(f"  TensorFlow : {tf.__version__}")
    print(f"  GPU(s)     : {[g.name for g in tf.config.list_logical_devices('GPU')] or 'None â€” CPU mode'}")
    print(f"  Batch size : {cfg.BATCH_SIZE}")
    print(f"  Max epochs : {cfg.EPOCHS}")
    print(f"  Image size : {cfg.IMG_H}Ã—{cfg.IMG_W}")
    print(f"  Backbone   : {'MobileNetV2 (Transfer)' if cfg.USE_PRETRAINED else 'Custom CNN'}")

    # â”€â”€ 1. Download â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    dataset_root = download_dataset()

    # â”€â”€ 2. Discover folders â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    train_dir, valid_dir, classes = find_data_dirs(dataset_root)
    num_classes = len(classes)
    print(f"\n  Classes ({num_classes}):")
    for i, c in enumerate(classes):
        print(f"    [{i:02d}] {c}")

    # â”€â”€ 3. Count images â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    n_train = sum(len(list(d.glob("*.*"))) for d in train_dir.iterdir() if d.is_dir())
    n_val   = sum(len(list(d.glob("*.*"))) for d in valid_dir.iterdir()  if d.is_dir()) if valid_dir else 0
    print(f"\n  Training images   : {n_train:,}")
    print(f"  Validation images : {n_val:,}")

    # â”€â”€ 4. Build tf.data pipelines â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("\n  [3/9] Building data pipelinesâ€¦")
    train_ds = build_dataset(train_dir, classes, training=True)
    val_ds   = build_dataset(valid_dir, classes, training=False) if valid_dir else None
    print("  âœ… Pipelines ready")

    # â”€â”€ 5. Build model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("\n  [4/9] Building modelâ€¦")
    model = build_model(num_classes)
    model.summary(line_length=90, expand_nested=False)
    print(f"\n  Total parameters : {model.count_params():,}")

    # â”€â”€ 6. Compile â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("\n  [5/9] Compilingâ€¦")
    steps_per_epoch = max(1, n_train // cfg.BATCH_SIZE)
    model = compile_model(model, steps_per_epoch)
    print("  âœ… Compiled")

    # â”€â”€ 7. Train â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("\n  [6/9] Trainingâ€¦")
    print("â•"*60)
    history = train(model, train_ds, val_ds or train_ds, n_train, n_val or n_train)

    # â”€â”€ 8. Evaluate â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    metrics = evaluate(model, val_ds or train_ds, n_val or n_train)

    # â”€â”€ 9. Plot â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("\n  [7/9] Saving training plotsâ€¦")
    plot_history(history, cfg.PLOT_DIR / "training_history.png")

    # â”€â”€ 10. Save artifacts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("\n  [8/9] Saving model & artifactsâ€¦")
    model_path = save_artifacts(model, classes, history, metrics)

    # â”€â”€ Done â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("\n  [9/9] Done! ğŸ‰")
    print("â•"*60)
    print(f"  âœ… Best val accuracy : {max(history.history.get('val_accuracy', [0]))*100:.2f}%")
    print(f"  âœ… Best top-3 acc    : {max(history.history.get('val_top3_acc',  [0]))*100:.2f}%")
    print(f"\n  Model ready for Streamlit:")
    print(f"    cp {model_path} saved_models/plant_health_final.keras")
    print(f"    streamlit run app.py")
    print("â•"*60 + "\n")

    return model, classes, history


if __name__ == "__main__":
    main()
